{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19a6508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60.88s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /Users/ishan/miniforge3/lib/python3.10/site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: torch==2.6.0 in /Users/ishan/miniforge3/lib/python3.10/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ishan/miniforge3/lib/python3.10/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in /Users/ishan/miniforge3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/ishan/miniforge3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/ishan/miniforge3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/ishan/miniforge3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/ishan/miniforge3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/ishan/miniforge3/lib/python3.10/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ishan/miniforge3/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ishan/miniforge3/lib/python3.10/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Downloading torchvision-0.21.0-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "Successfully installed torchvision-0.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "680d61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c019b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_pretrained_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac71eba3",
   "metadata": {},
   "source": [
    "# Data Pre Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3b2ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "all_labels = [os.path.basename(os.path.dirname(os.path.dirname(p))) for p in image_paths]\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "class NutritionDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        all_labels = [os.path.basename(os.path.dirname(os.path.dirname(p))) for p in image_paths]\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(all_labels)\n",
    "        self.encoder = label_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label_str = os.path.basename(os.path.dirname(os.path.dirname(img_path)))\n",
    "        label = self.encoder.transform([label_str])[0]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a9aaf777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image paths list...\n",
      "54845\n"
     ]
    }
   ],
   "source": [
    "# Create images paths\n",
    "print(\"Creating image paths list...\")\n",
    "dir_path = 'raw_images'\n",
    "\n",
    "image_paths = []\n",
    "\n",
    "for dish_dir in os.listdir(dir_path):\n",
    "    img_dir = os.path.join(dir_path, dish_dir, 'frames_sampled30')\n",
    "\n",
    "    if not os.path.exists(img_dir):\n",
    "        continue\n",
    "    \n",
    "    for image in os.listdir(img_dir):\n",
    "        img_path = os.path.join(img_dir, image)\n",
    "        image_paths.append(img_path)\n",
    "import random\n",
    "\n",
    "# Reduce dataset size for quicker experimentation\n",
    "# random.seed(42)\n",
    "# image_paths = random.sample(image_paths, min(10000, len(image_paths)))  # adjust 1000 to a smaller number if needed\n",
    "\n",
    "print(len(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cb1954ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating statistics (DO NOT RUN! WILL TAKE FOREVER)\n",
    "# stats_dataset = NutritionDataset(image_paths)\n",
    "# data_loader = DataLoader(stats_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# mean = 0.0\n",
    "# std = 0.0\n",
    "# total_images_count = 0\n",
    "\n",
    "# for images, _ in data_loader:\n",
    "#     batch_samples = images.size(0)\n",
    "#     images = images.view(batch_samples, images.size(1), -1)\n",
    "#     mean += images.mean(dim=2).sum(dim=0)\n",
    "#     std += images.std(dim=2).sum(dim=0)\n",
    "#     total_images_count += batch_samples\n",
    "\n",
    "# mean /= total_images_count\n",
    "# std /= total_images_count\n",
    "\n",
    "# print(f\"Mean: {mean}\")\n",
    "# print(f\"Std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "02bbd863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms/Data Augmentation\n",
    "\n",
    "input_size = (225, 225)\n",
    "\n",
    "if using_pretrained_model:\n",
    "    # Replace with the pretrained model's stats\n",
    "    data_normals = {\n",
    "        'mean': [0.485, 0.456, 0.406],\n",
    "        'std': [0.229, 0.224, 0.225]\n",
    "    }\n",
    "else:\n",
    "    # mean and std of the entire dataset\n",
    "    data_normals = {\n",
    "        'mean': [0.5005, 0.4726, 0.3732],\n",
    "        'std': [0.2193, 0.2296, 0.2398]\n",
    "    }\n",
    "    \n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(input_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.1\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=data_normals['mean'],\n",
    "        std=data_normals['std']\n",
    "    )\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=data_normals['mean'],\n",
    "        std=data_normals['std']\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ccc91407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader...\n",
      "torch.Size([32, 3, 225, 225]) 32\n",
      "torch.Size([32, 3, 225, 225]) 32\n",
      "torch.Size([32, 3, 225, 225]) 32\n",
      "torch.Size([32, 3, 225, 225]) 32\n",
      "torch.Size([32, 3, 225, 225]) 32\n",
      "Test Loader...\n",
      "torch.Size([32, 3, 225, 225]) 32\n",
      "torch.Size([32, 3, 225, 225]) 32\n",
      "torch.Size([32, 3, 225, 225]) 32\n",
      "torch.Size([32, 3, 225, 225]) 32\n",
      "torch.Size([32, 3, 225, 225]) 32\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.7 * len(image_paths))\n",
    "test_size = len(image_paths) - train_size\n",
    "train_imgs, test_imgs = random_split(image_paths, [train_size, test_size])\n",
    "\n",
    "train_set = NutritionDataset(train_imgs, transform=train_transform)\n",
    "test_set = NutritionDataset(test_imgs, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# Checking Train loader\n",
    "print(\"Train Loader...\")\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(images.shape, len(labels))\n",
    "\n",
    "# Checking Test loader\n",
    "print(\"Test Loader...\")\n",
    "for i, (images, labels) in enumerate(test_loader):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(images.shape, len(labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "836d17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "def extract_features(dataloader):\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, lbls in tqdm(dataloader):\n",
    "            flat_imgs = images.view(images.size(0), -1)\n",
    "            features.append(flat_imgs.numpy())\n",
    "            labels.append(lbls.numpy())\n",
    "    features = np.concatenate(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2647a1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [09:31<00:00,  2.10it/s]\n",
      "100%|██████████| 515/515 [04:19<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with kNN: 0.0009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_features, train_labels = extract_features(train_loader)\n",
    "test_features, test_labels = extract_features(test_loader)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(train_features, train_labels)\n",
    "test_preds = knn.predict(test_features)\n",
    "acc = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Test Accuracy with kNN: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee090f49",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknn.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m loc:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('knn.pkl', 'wb') as loc:\n",
    "    pickle.dump(knn, loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38edc993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "temp = set()\n",
    "i = 1\n",
    "for val, label in train_set:\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    temp.add(label)\n",
    "\n",
    "num_classes = len(temp)\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    i = 0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        i += 1\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        temp, preds = outputs.max(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    train_acc = correct / len(train_set)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = outputs.max(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "test_acc = correct / len(test_set)\n",
    "print(f\"Test Accuracy with ResNet: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da3fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('resnet.pkl', 'wb') as loc:\n",
    "    pickle.dump(model, loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
